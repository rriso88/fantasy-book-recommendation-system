{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Book IDs for Fantasy Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install goodreads-api-client\n",
    "#!pip install xmltodict\n",
    "#import goodreads_api_client as gr\n",
    "#from goodreads import client\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import xmltodict\n",
    "import pprint\n",
    "import json\n",
    "import os\n",
    "#!pip install selenium\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "#chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "#os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define helpful functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(url):\n",
    "    key = api_key\n",
    "    response = requests.get(url, params={'key': api_key})\n",
    "    #soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "    results = xmltodict.parse(response.content)\n",
    "    #results = json.dumps(results)\n",
    "    return(results['GoodreadsResponse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    #print(soup.prettify())\n",
    "    return(soup)\n",
    "\n",
    "def get_soup_selenium(url):\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    return(soup)\n",
    "def stripHTML(v):\n",
    "    v = [re.sub(r'\\t', '', x) for x in v]\n",
    "    v = [re.sub(r'\\r', '', x) for x in v]\n",
    "    v = [re.sub(r'\\n', '', x) for x in v]\n",
    "    return(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(soup):\n",
    "    count = soup.find_all('div',\n",
    "                          {'class': 'inter loading uitext'})[1].text.split('of ')[1].split(' loaded')[0]\n",
    "\n",
    "\n",
    "    \n",
    "def get_maxpage(soup):\n",
    "    page_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "            #links.append(link['href'])\n",
    "        try:\n",
    "            if '?page=' in str(link['href']):\n",
    "                page_links.append(str(link['href']))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    max_page = page_links[-2].split('?page=')[1]\n",
    "    return(max_page)\n",
    "    #print(max_page, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userids = pd.read_pickle('user_ids.pkl')\n",
    "len(userids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6730"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_all_users = pd.read_pickle('user_book_ratings_scraped.pkl')\n",
    "len(ratings_all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last user scraped is: 11454689 This is last index of user_ids: 10997\n"
     ]
    }
   ],
   "source": [
    "print('The last user scraped is:', ratings_all_users[-1]['userid'], \n",
    "      'This is last index of user_ids:', userids.index(ratings_all_users[-1]['userid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters = userids.index(ratings_all_users[-1]['userid'])\n",
    "iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building users' ratings dict with web scraping. Starting with user 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10998 list index out of range\n",
      "10999\n",
      "11000 list index out of range\n",
      "11001\n",
      "11002 list index out of range\n",
      "11003 list index out of range\n",
      "11004\n",
      "11005\n",
      "11006 list index out of range\n",
      "11007 list index out of range\n",
      "11008 list index out of range\n",
      "11009\n",
      "11010\n",
      "11011 list index out of range\n",
      "11012 list index out of range\n",
      "11013\n",
      "11014 list index out of range\n",
      "11015\n"
     ]
    }
   ],
   "source": [
    "#ratings_all_users = [] # [{userid: gid, ratings: [{bookid: bookid, rating: stars}]}]\n",
    "#for gid in userids[0:1]:\n",
    "for count, gid in enumerate(userids[iters+1:]):  \n",
    "    ratings_user_i = {} # user_ratings_dict = {'userid':gid, 'ratings': user_books_rating_list}\n",
    "    ratings_user_i['userid'] = gid\n",
    "    \n",
    "    try:\n",
    "        soup1 = get_soup('https://www.goodreads.com/review/list/'+gid)\n",
    "        max_page = get_maxpage(soup1)  \n",
    "\n",
    "        user_ratings_all = []\n",
    "\n",
    "        for page in range(1, int(max_page)):\n",
    "\n",
    "            url = 'https://www.goodreads.com/review/list/'+gid+'?page='+str(page)\n",
    "            soup = get_soup(url)\n",
    "\n",
    "            tables = soup.find_all(\"table\")\n",
    "            #tables[1]\n",
    "            rows = tables[1].find_all('tr')\n",
    "\n",
    "            tds = [row.find_all('td') for row in rows]\n",
    "\n",
    "            for i, td in enumerate(tds):\n",
    "                user_ratings = {}\n",
    "                try:\n",
    "                    user_ratings['rating'] = tds[i][13].find('span', {'class': 'staticStars'}).text\n",
    "                    user_ratings['bookid'] = tds[i][2].find('div', \n",
    "                                    {'class': \"js-tooltipTrigger tooltipTrigger\"})['data-resource-id']\n",
    "                    user_ratings_all.append(user_ratings)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        ratings_user_i['ratings'] = user_ratings_all\n",
    "\n",
    "        ratings_all_users.append(ratings_user_i)\n",
    "\n",
    "        if count % 50 == 0:\n",
    "            with open('user_book_ratings_scraped.pkl', 'wb') as f:\n",
    "                pickle.dump(ratings_all_users, f)\n",
    "        print(count+iters+1)\n",
    "    except Exception as e:\n",
    "        print(count+iters+1, e)\n",
    "        #pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ratings_all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_book_ratings_scraped.pkl', 'wb') as f:\n",
    "    pickle.dump(ratings_all_users, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END NOTEBOOK**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
